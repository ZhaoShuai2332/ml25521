# ğŸš€ ml25521

It's Friday afternoon now.

---

## ğŸ“‹ Table of Contents

- [ğŸš€ ml25521](#-ml25521)
  - [ğŸ“‹ Table of Contents](#-table-of-contents)
  - [âœ¨ Features](#-features)
  - [ğŸ—‚ Repository Layout](#-repository-layout)
  - [âš¡ Quick-Start](#-quick-start)
    - [1ï¸âƒ£â€ƒClone](#1ï¸âƒ£clone)
    - [2ï¸âƒ£â€ƒCreate a Python 3.10+ virtual environment](#2ï¸âƒ£create-a-python-310-virtual-environment)
    - [3ï¸âƒ£â€ƒInstall requirements](#3ï¸âƒ£install-requirements)
  - [ğŸ‹ï¸â€â™€ï¸ Model Training](#ï¸ï¸-model-training)
  - [ğŸ”„ Loading Saved Artifacts](#-loading-saved-artifacts)
  - [ğŸ›  Dataset Utilities](#-dataset-utilities)
  - [ğŸŒ Data Source](#-data-source)

---

## âœ¨ Features

- **Two classic regressors** â€“ plain Linear Regression (`models/linear.py`) and Support Vector Regression (`models/svr.py`).  
- **Three datasets** â€“ MNIST (sub-sampled to regression targets), a cleaned UCI bundle, and a synthetic Credit dataset.  
- **One-command training** that serialises both model weights and feature scalers to `model_params/`.  
- **Minimal dependency set** (see `piplist.txt`) to keep the environment footprint small.

---

## ğŸ—‚ Repository Layout

```
ml25521/
â”œâ”€â”€ data/                # Datasets (not tracked in Git)
â”œâ”€â”€ model_params/        # Saved *.npz weights & scalers
â”œâ”€â”€ model_test/
â”‚   â””â”€â”€ commends/        # Evaluation & data-loading helpers
â”œâ”€â”€ models/              # Training scripts (linear.py, svr.py)
â”œâ”€â”€ piplist.txt          # Python dependencies
â””â”€â”€ README.md            # Project documentation
```

---

## âš¡ Quick-Start

### 1ï¸âƒ£â€ƒClone

```bash
git clone https://github.com/ZhaoShuai2332/ml25521.git
cd ml25521
```

### 2ï¸âƒ£â€ƒCreate a Python 3.10+ virtual environment

```bash
python -m venv env
# Linux / macOS
source env/bin/activate
# Windows
env\Scripts\activate
```

### 3ï¸âƒ£â€ƒInstall requirements

```bash
pip install --upgrade pip
pip install -r piplist.txt
```

---

## ğŸ‹ï¸â€â™€ï¸ Model Training

```bash
python models/<model_name>.py --name <dataset_name>
```

| `<model_name>` | `<dataset_name>` |
| -------------- | ---------------- |
| `linear`       | `mnist` \| `uci` \| `credit` |
| `svr`          | `mnist` \| `uci` \| `credit` |

Artifacts are written to `model_params/` as:

```
{dataset_name}_{model_name}_params.npz
{dataset_name}_scaler_params.npz
```

---

## ğŸ”„ Loading Saved Artifacts

```python
import os, numpy as np

root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))

params = np.load(
    os.path.join(root, "model_params",
                 f"{dataset_name}_{model_name}_params.npz")
)

scaler_params = np.load(
    os.path.join(root, "model_params",
                 f"{dataset_name}_scaler_params.npz")
)
```

---

## ğŸ›  Dataset Utilities

```python
import os, sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from model_test.commends import dataset_selecter

train_X, test_X, train_y, test_y = dataset_selecter(name="mnist")
```

Valid dataset names: `mnist`, `uci`, `credit`.

---

## ğŸŒ Data Source

Large raw datasets reside in **`data/datasets/`** but are *excluded* from version control to keep the repository lightweight.

But you can access them on my [onedrive](https://stummuac-my.sharepoint.com/:f:/r/personal/21901260_stu_mmu_ac_uk/Documents/datasets?csf=1&web=1&e=1Ki0Pk).

